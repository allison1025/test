---
title: "Machine Learning Analysis of College Admissions"
author: "Allison Yih and Emi Kong"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  word_document: default
  pdf_document: default
---
#Introduction
Many high school students stress out about college admissions, often wondering what their chances are for getting into a certain university. It is common for someone's  peers, who have lower testing scores, to get into a college that that individual is rejected from. 

To get a better idea of what college admissions officers view as the most important factors of a successful applicant, we developed a supervised, binary classifier that predicts an individual's admission outcome for our project. 

The applicant's weighted high school GPA, standardized testing scores, and family income are the factors taken into consideration. Through building this model, this can help college applicants figure out which aspects of their application should be focused on in order to maximize their likelihood of acceptance.

#Data Overview
The dataset that will be used for analyzing this college's admissions is AdmissionsData.rdata, which is from a data scientist, Ernest Tavares's, blog. In this dataset, there are 8700 observations that describe each applicant's academic scores, personal background, and admissions decision. The variable "admit" is the binary response variable, which takes on the value 0 and 1 to represent getting a rejection and acception, respectively. 

Our dataset contained 2196 rows with 0 values under the columns "sati.verb" and "sati.math". This could be because those individuals took the ACT, which is the alternative exam for the SAT, and those scores were not reported. By omitting the applicants with missing data, this ensured that we were only looking at the data for applicants who took the SAT exam. 

Our cleaned dataset has a dimension of 6182 rows x 5 columns. The columns include "admit" and 4 numeric predictor variables: "gpa.wtd", "sati.verb", "sati.math", and "income". The variables "anglo", "asian", and "black" were omitted in our analysis since there was an initial uneven racial distribution of all applicants, and "sex" was excluded because the amount of gender bias in college admissions differs between college types, according to an article by the Washington Post.

By removing missing values and these 4 predictor variables, this allowed for more consistency when comparing classification methods and test error rates, since some of the methods we used could not handle categorical predictors.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("/Users/allisonyih/Desktop/AdmissionsData.rdata")
#load('/Users/emi/UCSB\ Coding/pstat131/AdmissionsData.rdata') 
library(dplyr)
library(class) #for knn classifiers
library(ROCR) #for log regression (ROC)
```



```{r}
admissions_data <- Admissions
admissions_new <- na.omit(admissions_data) 
admissions_new <- filter(admissions_new, sati.math !=0, sati.verb != 0)
#colSums(admissions_new)
admissions_new <- subset(admissions_new, select = c(admit, gpa.wtd, sati.verb, sati.math, income)) 
head(admissions_new)
summary(admissions_new)
dim(admissions_new)
```

```{r}
#convert admit variable into factors
admissions_new$admit<-as.factor(admissions_new$admit)
str(admissions_new)
```

#Methods
To find the model that would accurately classify whether an applicant gets admitted to this college, we will apply supervised machine learning methods on a training dataset.

#Exploratory Data Analysis
We start by loading the dataset into R and plotting box plots and histograms to investigate any significant relationships between the predictor variables and admittance. 

```{r, out.width='50%', fig.show='hold'}
boxplot(gpa.wtd ~ admit, data = admissions_new, main = "Weighted GPA", 
        ylab = "weighted gpa")
boxplot(sati.verb ~ admit, data = admissions_new, main = "Verbal SAT Scores",
        ylab = "verbal SAT scores")
boxplot(sati.math ~ admit, data = admissions_new, main = "Math SAT Scores", 
        ylab = "math SAT scores")
boxplot(income ~ admit, data = admissions_new, main = "Income", 
        ylab = "income")
```
From these boxplots, we see that the people who are admitted have higher weighted GPAs, and higher verbal and math SAT scores. There is little difference in income amount for people who are admitted versus people who are not admitted. 

Next, we plot histogram graphs to see the distibution of each predictor for all applicants separated by color (depending on if they were admitted or not).

```{r, out.width='50%', fig.show='hold'}
library(ggplot2)
ggplot(admissions_new, aes(x=gpa.wtd, fill = admit, color=admit)) + geom_histogram(bins=30) +
  ggtitle("Weighted GPA")
ggplot(admissions_new, aes(x=sati.verb, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Verbal SAT Scores")
ggplot(admissions_new, aes(x=sati.math, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Math SAT Scores")
ggplot(admissions_new, aes(x=income, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Income")
```

For weighted GPA, an applicants chances of getting admitted are extremeely high if their GPA is around 4.25 or higher. This is also seen when applicants have a verbal SAT score of at least 700 and/or a math SAT score around 725 or greater. Since a majority of the applicants have income around $100,000 it is expected that the majority of people admitted also have that same income amount.

#Splitting the data into Training and Testing Sets
Then, we split the dataset into equally-sized training and testing sets. The first 3091 observations would be used to identify and construct our model, and the remaining half would be used to determine whether our chosen classifier gave highly accurate predictions.

```{r}
set.seed(50)
#50% train, 50% test
cutoff <- sort(sample(nrow(admissions_new), nrow(admissions_new)*.5))
train <- admissions_new[cutoff,]
test <- admissions_new[-cutoff,]
```

The algorithms that we will use to classify our data observations are decision trees, random forest, K-nearest neighbors, and logistic regression. Leave-one-out cross validation will also be used to tune parameters for KNN in order to reduce bias and test error rate. For each algorithm, we will calculate a confusion matrix about predicted vs actual admit results, and compare their test accuracy rates. By having our model learn and identify college acceptance patterns from previous data, we hope that it will help our model more accurately predict an outcome on new, similar data.

#K-Nearest Neighbors
Now that we split our data into 50% training and 50% testing, we first chose the K-nearest neighbors method to analyze the relationship between admissions and all numerical predictors. We created response vectors & design matrices to train the KNN classifier with 5, 10, and 20 neighbors initially. We chose these values because they are common K’s that tend to optimize the bias-variance tradeoff. Using leave one out cross validation (LOOCV), we also found the best value of K was 35 neighbors and chose to train the KNN classifier with this value as well. 

**Creating Response Vectors & Design Matrices:**
```{r}
#YTrain is the observed results for admit (0 = not admitted, 1 = admitted)
YTrain <- train$admit
#XTrain is the design matrix after removing admit variable and standardizing 
XTrain <- train[ ,-which(names(train) %in% c("admit"))] %>% 
  scale(center = TRUE, scale = TRUE)

#mean and standard deviation of training set
meanvec <- attr(XTrain,'scaled:center') 
sdvec <- attr(XTrain,'scaled:scale')

#X and Y for testing set
YTest <- test$admit
XTest <- test[ ,-which(names(train) %in% c("admit"))] %>% 
  scale(center = meanvec, scale = sdvec)
```

**Making Predictions on the Test Set with K = 5, 10, 15**
```{r}
#k=5
pred5 = knn(train=XTrain, test=XTest, cl=YTrain, k=5) 
conf.matrix5 <- table(pred=pred5, true=YTest) 
err5 = 1 - sum(diag(conf.matrix5)/sum(conf.matrix5))
#k=10
pred10 = knn(train=XTrain, test=XTest, cl=YTrain, k=10) 
conf.matrix10 <- table(pred=pred10, true=YTest) 
err10 = 1 - sum(diag(conf.matrix10)/sum(conf.matrix10))
#k=20
pred15 = knn(train=XTrain, test=XTest, cl=YTrain, k=15) 
conf.matrix15 <- table(pred=pred15, true=YTest) 
err15 = 1 - sum(diag(conf.matrix15)/sum(conf.matrix15))
```

**Using LOOCV to Find the Best Value of K:**
```{r}
set.seed(150)
#define an empty vector to save validation errors in future
validation.error = NULL
#give possible number of nearest neighbours to be considered 
allK = 1:50
#for each number in allK, use LOOCV to find a validation error 
for (i in allK){
  pred.Yval <- knn.cv(train=XTrain, cl=YTrain, k=i)
  validation.error <- c(validation.error, mean(pred.Yval!=YTrain)) 
}
#best number of neighbors
bestK <- max(allK[validation.error == min(validation.error)]) 
bestK
```

The best value of K to use is 35 because it results in the lowest training error rate.

```{r}
#make predictions on the test set with bestK
pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=bestK) 
#confusion matrix
conf.matrix.knn <- table(pred=pred.YTest, true=YTest) 
conf.matrix.knn
#test error rate
err_bestk = 1 - sum(diag(conf.matrix.knn)/sum(conf.matrix.knn))
err_bestk
```

**Comparing Test Error Rates**
```{r}
knn_results <- matrix(c(err5, err10, err15, err_bestk), ncol=1, byrow=TRUE)
colnames(knn_results) <- c("Test Error Rate")
rownames(knn_results) <- c("K = 5","K = 10","K = 15", "K = 35")
knn_results.table <- as.table(knn_results)
knn_results.table
```

After training all KNN classifiers, we fit the test sets to each model and calculated the test error rates. The model based on 35 nearest neighbors resulted in the lowest misclassification error rate on the test set at 0.1633775. This makes sense since the purpose of LOOCV is to help us find the optimal number of neighbors, and since larger values of K produce smoother decision boundaries with lower variance.

#Logistic Regression

The second basic classifying method we used was logistic regression. Logistic regression is useful when determining variable importance by showing which predictors have the largest impact on our response (admission). 

```{r}
#fit
train.glm <- glm(admit ~ ., data=train, family=binomial) 
summary(train.glm)
```

From the summary output, we see that all four of our predictors are significant to an applicant’s admission based on their low p-values. For a one unit increase in each of these predictor values, the log-odds of getting admitted increases by the specific coefficient amount of that predictor when holding other predictors constant. For example, for a one unit increase in weighted GPA of an applicant, the log-odds of being admitted increases by 3.34.


**Checking How Accurate our Model is with ROC & AUC**

After fitting our logistic regression model on the training set, we constructed a Receiver Operating Characteristic (ROC) curve by plotting the true positive rate (TPR) against the false positive rate (FPR) of our soft classifier and calculated the area under this curve (AUC).

```{r}
train.glm.pred <- predict(train.glm, type="response")
pred = prediction(train.glm.pred, train$admit)
perf = performance(pred, measure="tpr", x.measure="fpr")
#ROC
plot(perf, col=2, lwd=3, main="ROC curve") 
abline(0,1)
#AUC
performance(pred, "auc")@y.values 
```

Since our curve (red line) follows close to the top-left corner of the graph and our AUC, 0.8861, is close to 1, we conclude that logistic regression is a very good classifier for this dataset. If the curve was to closely follow the diagonal (FPR=TPR) and have an area close to 0.5, this would indicate an inaccurate model.

**Using FPR & FNR to Find the Best Threshold Value**

To calculate the test misclassification error rate for this model we need to change the soft classifier (probability values) to a hard classifier (1 or 0) by setting a specific threshold value to assign the labels. We will use our false positive (FPR) and false negative (FNR) results to do this. While still using the training set logistic regression model, we calculated the euclidean distance between each point of (FPR,FNR) and (0, 0).

```{r}
fpr = performance(pred, "fpr")@y.values[[1]] 
threshold = performance(pred, "fpr")@x.values[[1]] 
fnr = performance(pred,"fnr")@y.values[[1]]
#calculating euclidean distance
rate = as.data.frame(cbind(Cutoff=threshold, FPR=fpr, FNR=fnr)) 
rate$distance = sqrt((rate[,2])^2+(rate[,3])^2)
#select threshold with the smallest euclidean distance
index = which.min(rate$distance) 
best = rate$Cutoff[index]
best
```

The best threshold value to use is 0.347139.

```{r}
#plot
matplot(threshold, cbind(fpr,fnr), type="l",lwd=2, xlab="Threshold",ylab="Error Rate") 
legend(0.38, 1, legend=c("False Positive Rate","False Negative Rate"),col=c(1,2), lty=c(1,2))
abline(v=best, col=3, lty=3, lwd=3)
```

```{r}
test.glm <- glm(admit ~ ., data=test, family=binomial)
#soft classifier
test.glm.pred <- predict(test.glm, type="response")
#hard classifier w/ "best" threshold 
test_new = test %>% mutate(admit_pred=as.factor(ifelse(test.glm.pred<=best, 0, 1)))
#confusion matrix
conf.matrix.log <- table(pred=test_new$admit_pred, true=test$admit)
#test error rate
log_error <- 1 - sum(diag(conf.matrix.log)/sum(conf.matrix.log))
log_error
```

With our optimal threshold value, we fit our testing set logistic regression model, constructed a confusion matrix, and found our test misclassification error rate to be 0.1844063. This is slightly greater than our test error rate for K-nearest neighbors.


#Decision Trees
The decision tree algorithm was used because its output is simple to interpret. Each split represents the order in which the tree decides how to further split a set of features to eventually classify a qualitative response. 

To implement the decision tree, we first fit our model with all 4 predictors on the training set. 
```{r}
library(tree)
tree_fit <- tree(admit ~ ., data = train) #fitting classification tree
plot(tree_fit) #plotting tree
text(tree_fit, pretty = 0)
title("Classification Tree Built on Training Set") 
```
However, based on the decision tree’s plot, the data was split using only the gpa.wtd and sati.verb variables. Since decision trees involve looking at each variable’s importance, this indicates that gpa.wtd, followed by sati.verb, were the most significant variables that would help our model gain the most information regarding how to make the best split at each node. 

To calculate the test error rate for our classification tree, we predicted the response values for the test set and then constructed a confusion matrix. 
```{r}
admit.testset <- test$admit
#predict each observation's class using the test set
yhat.testset <- predict(tree_fit, test, type="class") 

#confusion matrix
test_tree_matrix <- table(yhat.testset, admit.testset)
test_tree_matrix
```

Finally, we subtracted the test accuracy rate, which is found through dividing the counter diagonal sum by the total sum, from 1 to get the test error rate.
```{r}
#finding test error rate for this tree
tree_error_rate <- 1-sum(diag(test_tree_matrix))/sum(test_tree_matrix)
tree_error_rate
```
Our decision tree's misclassification error rate is 0.1779. This means that our classification tree was able to correctly predict the admissions decision for 82.21% of the observations in the test set. When compared to KNN, our classification tree algorithm had a slightly higher test error rate, but had a smaller test error rate than logistic regression.

#Random Forest

Another method that was used for our classification was random forest. While containing multiple decision trees makes them harder to interpret, random forests prevent data overfitting and give highly accurate predictions, due to their robustness as an ensemble learning method. 

To implement the classification tree version of a random forest, we fit the model and set the number of parameters as the square root of the total number of predictors. 
```{r}
library(randomForest)
library(tree)
library(gbm)

rf_admits = randomForest(admit ~ ., data=train, ntree=500, importance = TRUE)
plot(rf_admits)
print(rf_admits)
```

Then, we constructed a confusion matrix and calculated the misclassification rate to be 0.1715.
```{r}
yhat.rf = predict(rf_admits, newdata = test)
rf_error = table(pred = yhat.rf, truth = test$admit)
test_rf_error = 1 - sum(diag(rf_error))/sum(rf_error)
test_rf_error
```

Another advantage of using random forests is that they provide information about not only the strength of the relationship between each predictor and response, but also how influential each predictor is on the classifications. 

Each feature's importance is based off of their mean decrease in accuracy and Gini index. To view the degree of influence that each predictor has on the result, we use the importance() function, which displays these 2 factors in decreasing order. 
```{r}
importance(rf_admits)
```
From this output, it tells us that most important variable for getting admitted into the college was weighted GPA, as it had the highest decrease in mean accuracy and Gini index. Verbal SAT score was the 2nd-most important factor that determined the individual's admission decision.

Additionally, we plotted each variable's mean decreases in accuracy and Gini index. 
```{r}
varImpPlot(rf_admits, sort = T, main = "Variable Importance Plot of College Admission Factors")
```
From those graphs, it reemphasizes that the "gpa.wtd" variable had the most influence over the final admissions decision across all trees in the random forest the most, in terms of model accuracy and Gini index.

Overall, random forest had the 2nd-lowest misclassification rate, but did not perform as well as the KNN LOOCV method. 