---
title: "Machine Learning Analysis of College Admissions"
author: "Allison Yih and Emi Kong"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#load("/Users/allisonyih/Desktop/AdmissionsData.rdata")
load('/Users/emi/UCSB\ Coding/pstat131/AdmissionsData.rdata') 
library(dplyr)
library(class) #for knn classifiers
library(MASS) #for LDA
```

#Data Pre-Processing

```{r}
admissions_data <- Admissions
admissions_new <- na.omit(admissions_data) 
admissions_new <- subset(admissions_new, select = c(admit, gpa.wtd, sati.verb, sati.math, income)) 
admissions_new <- filter(admissions_new, sati.math !=0, sati.verb != 0)
head(admissions_new)
summary(admissions_new) 
```

```{r}
#convert admit variable into factors
admissions_new$admit<-as.factor(admissions_new$admit)
str(admissions_new)
```

#Exploratory Data Analysis
Note that, before 2005, the SAT had only two sections: Verbal and Math. Each section was scored on a scale of 200-800 with the composite SAT score ranging from 400-1600.
```{r, out.width='50%', fig.show='hold'}
boxplot(gpa.wtd ~ admit, data = admissions_new, main = "Weighted GPA", 
        ylab = "weighted gpa")
boxplot(sati.verb ~ admit, data = admissions_new, main = "Verbal SAT Scores",
        ylab = "verbal SAT scores")
boxplot(sati.math ~ admit, data = admissions_new, main = "Math SAT Scores", 
        ylab = "math SAT scores")
boxplot(income ~ admit, data = admissions_new, main = "Income", 
        ylab = "income")
```

We start by investigating any significant relationships between our predictors and admittance. From these boxplots, we see that the people who are admitted have higher weighted GPAs, and higher verbal and math SAT scores. There is little difference in income amount for people who are admitted versus people who are not admitted. 

```{r, out.width='50%', fig.show='hold'}
library(ggplot2)
ggplot(admissions_new, aes(x=gpa.wtd, fill = admit, color=admit)) + geom_histogram(bins=30) +
  ggtitle("Weighted GPA")
ggplot(admissions_new, aes(x=sati.verb, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Verbal SAT Scores")
ggplot(admissions_new, aes(x=sati.math, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Math SAT Scores")
ggplot(admissions_new, aes(x=income, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Income")
```

Next, we plot histogram graphs to see the distibution of each predictor for all applicants separated by color (depending on if they were admitted or not). For weighted GPA, an applicants chances of getting admitted are extremeely high if their GPA is around 4.25 or higher. This is also seen when applicants have a verbal SAT score of at least 700 and/or a math SAT score around 725 or greater. Since a majority of the applicants have income around $100,000 it is expected that the majority of people admitted also have that same income amount.

#Splitting the data into 50% training and 50% testing
```{r}
set.seed(50)
cutoff <- sort(sample(nrow(admissions_new), nrow(admissions_new)*.5))
train <- admissions_new[cutoff,]
test <- admissions_new[-cutoff,]
```

#knn (emi)
CREATING RESPONSE VECTORS & DESIGN MATRICES:
```{r}
#YTrain is the observed results for admit (0 = not admitted, 1 = admitted)
YTrain <- train$admit
#XTrain is the design matrix after removing admit variable and standardizing 
XTrain <- train[ ,-which(names(train) %in% c("admit"))] %>% 
  scale(center = TRUE, scale = TRUE)

#mean and standard deviation
meanvec <- attr(XTrain,'scaled:center') 
sdvec <- attr(XTrain,'scaled:scale')

#X and Y for testing set
YTest <- test$admit
XTest <- test[ ,-which(names(train) %in% c("admit"))] %>% 
  scale(center = meanvec, scale = sdvec)
```


USING LOOCV TO FIND THE BEST NUMBER OF NEIGHBORS (K):
```{r}
#define an empty vector to save validation errors in future
validation.error = NULL
#give possible number of nearest neighbours to be considered 
allK = 1:50
#for each number in allK, use LOOCV to find a validation error 
for (i in allK){
  pred.Yval <- knn.cv(train=XTrain, cl=YTrain, k=i)
  validation.error <- c(validation.error, mean(pred.Yval!=YTrain)) 
}
#best number of neighbors
bestK <- max(allK[validation.error == min(validation.error)]) 
bestK
```

The best value of K to use is 35 because it results in the lowest training error rate.

```{r}
#make predictions on the test set with bestK
pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=bestK) 
#confusion matrix
conf.matrix.knn <- table(pred=pred.YTest, true=YTest) 
conf.matrix.knn
#test error rate
1 - sum(diag(conf.matrix.knn)/sum(conf.matrix.knn))
```


#lda (emi)
```{r}
#fit
admit.lda <- lda(admit ~ ., data=train)
#yhat
admit.lda.pred <- predict(admit.lda, data=test)$class
#confusion matrix
conf.matrix.lda <- table(pred=admit.lda.pred, true=test$admit) 
conf.matrix.lda
#test error rate
1 - sum(diag(conf.matrix.lda)/sum(conf.matrix.lda))
```

#qda (emi)
```{r}
#fit
admit.qda <- qda(admit ~ ., data=train)
#yhat
admit.qda.pred <- predict(admit.qda, data=test)$class
#confusion matrix
conf.matrix.qda <- table(pred=admit.qda.pred, true=test$admit) 
conf.matrix.qda
#test error rate
1 - sum(diag(conf.matrix.qda)/sum(conf.matrix.qda))
```

#log reg (emi)
```{r}
#fit
admit.glm <- glm(admit ~ ., data=train, family=binomial) 
summary(admit.glm)
#phat (soft class)
admit.glm.pred1 <- predict(admit.glm, data=test, type="response")
#yhat (hard class) w/ threshold 0.5
admit.glm.pred2 <- rep(0, length(admit.glm.pred1)) #create new vector w/ all prob 0 
admit.glm.pred2[admit.glm.pred1 > 0.5] <- 1 #set those w/ prob > 0.5 to 1 into the new vector
#confusion matrix
conf.matrix.log <- table(pred=admit.glm.pred2, true=test$admit) 
conf.matrix.log
#test error rate
1 - sum(diag(conf.matrix.log)/sum(conf.matrix.log))
```

#decision trees (allison)
```{r}
library(tree)
#fit <- tree(admit ~., data = train)
#summary(fit)
#plot(fit)
#text(fit, pretty = 0, cex = .7)
#title("Decision Tree Built on Training Set")
```

#random forest (allison)
```{r}
library(randomForest)
library(tree)
library(gbm)

rf_admits = randomForest(admit ~ ., data=train, ntree=500, importance = TRUE)
plot(rf_admits)
print(rf_admits)
```

```{r}
yhat.rf = predict(rf_admits, newdata = test)
rf_error = table(pred = yhat.rf, truth = test$admit)
test_rf_error = 1 - sum(diag(rf_error))/sum(rf_error)
test_rf_error
```

```{r}
importance(rf_admits)
```
the most important variable for getting admitted into the college was weighted GPA. Verbal SAT score was the 2nd-most important factor that determined the individual's admission decision.