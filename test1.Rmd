---
title: "Machine Learning Analysis of College Admissions"
author: "Allison Yih and Emi Kong"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---

#Data Overview
For our project, we hope to develop a binary classification model that classifies whether someone gets admitted into an elite university, given that individual’s weighted high school GPA, standardized testing scores, and their family income

The dataset that will be used for analyzing this college's admissions is AdmissionsData.rdata, which is from a data scientist, Ernest Tavares's, blog. In this dataset, there are 8700 observations, 1 response variable, and 8 predictor variables, with the variables "admit" and "sex" being coded as binary categorical variables. The dataset contains information about each applicant's academic scores, personal background, and the type of admissions decision that they receive.

The variable "admit" is a factor and the response variable for our model. It takes on the value 0 and 1 to represent getting a rejection and acceptance, respectively. The variable "gpa.wtd" is a numeric predictor that describes what the individual's weighted high school grade point average is. Before 2005, the SAT had only two sections: Verbal and Math. Each section was scored on a scale of 200-800 with the composite SAT score ranging from 400-1600. The variable "sati.verb" is a integer predictor that describes what the individual's SAT verbal test score is. Like "sati.verb", "sati.math" is a integer type and describes what the individual's SAT math test score is. The variable "income" is also an integer variable, and it describes how much money that the individual's family makes per year. The variables "anglo", "asian", and "black" were omitted in our analysis since there was an initial uneven racial distribution of all applicants with 33% anglo, 45% asian, 4% black, and the remaining 18% unspecified. The “sex” categorical variable was omitted from the dataset because the amount of gender bias in college admissions differs between college types, according to an article by the Washington Post. For instance, small liberal arts schools admit 3% more males, while engineering-focused schools tend to admit women by 10-25% more. Because we don’t know the kind of college that these students applied to, removing “sex” from our analysis allowed for more consistency when comparing classification methods and test error rates since some of the methods we used couldn’t handle categorical predictors.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("/Users/allisonyih/Desktop/AdmissionsData.rdata")
#load('/Users/emi/UCSB\ Coding/pstat131/AdmissionsData.rdata') 
library(dplyr)
library(class) #for knn classifiers
library(ROCR) #for log regression (ROC)
```

#Data Preprocessing

After omitting the applicants with missing data, we noticed there were several rows under "sati.verb" and "sati.math" that had values of 0. This could be because those individuals took the ACT, which is the alternative exam for the SAT, and those scores were not reported. Because of the 0s in the SAT variable sections, we omitted all rows under "sati.verb" and "sati.math" having a value of 0. By removing those 2196 rows with missing or 0 values, this ensured that we were only looking at the data for applicants who took the SAT exam. Cleaning this dataset resulted in 6182 observations, 0 missing values, and 4 predictor variables.


```{r}
admissions_data <- Admissions
admissions_new <- na.omit(admissions_data) 
admissions_new <- filter(admissions_new, sati.math !=0, sati.verb != 0)
#colSums(admissions_new)
admissions_new <- subset(admissions_new, select = c(admit, gpa.wtd, sati.verb, sati.math, income)) 
head(admissions_new)
summary(admissions_new) 
```

```{r}
#convert admit variable into factors
admissions_new$admit<-as.factor(admissions_new$admit)
str(admissions_new)
```

#Exploratory Data Analysis

We start by investigating any significant relationships between our predictors and admittance.

```{r, out.width='50%', fig.show='hold'}
boxplot(gpa.wtd ~ admit, data = admissions_new, main = "Weighted GPA", 
        ylab = "weighted gpa")
boxplot(sati.verb ~ admit, data = admissions_new, main = "Verbal SAT Scores",
        ylab = "verbal SAT scores")
boxplot(sati.math ~ admit, data = admissions_new, main = "Math SAT Scores", 
        ylab = "math SAT scores")
boxplot(income ~ admit, data = admissions_new, main = "Income", 
        ylab = "income")
```
From these boxplots, we see that the people who are admitted have higher weighted GPAs, and higher verbal and math SAT scores. There is little difference in income amount for people who are admitted versus people who are not admitted. 

Next, we plot histogram graphs to see the distibution of each predictor for all applicants separated by color (depending on if they were admitted or not).

```{r, out.width='50%', fig.show='hold'}
library(ggplot2)
ggplot(admissions_new, aes(x=gpa.wtd, fill = admit, color=admit)) + geom_histogram(bins=30) +
  ggtitle("Weighted GPA")
ggplot(admissions_new, aes(x=sati.verb, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Verbal SAT Scores")
ggplot(admissions_new, aes(x=sati.math, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Math SAT Scores")
ggplot(admissions_new, aes(x=income, fill = admit, color=admit)) + geom_histogram(bins=30) + 
  ggtitle("Income")
```

For weighted GPA, an applicants chances of getting admitted are extremeely high if their GPA is around 4.25 or higher. This is also seen when applicants have a verbal SAT score of at least 700 and/or a math SAT score around 725 or greater. Since a majority of the applicants have income around $100,000 it is expected that the majority of people admitted also have that same income amount.

#Splitting the data into Training and Testing Sets
```{r}
set.seed(50)
#50% train, 50% test
cutoff <- sort(sample(nrow(admissions_new), nrow(admissions_new)*.5))
train <- admissions_new[cutoff,]
test <- admissions_new[-cutoff,]
```

#K-Nearest Neighbors
Now that we split our data into 50% training and 50% testing, we first chose the K-nearest neighbors method to analyze the relationship between admissions and all numerical predictors. We created response vectors & design matrices to train the KNN classifier with 5, 10, and 20 neighbors initially. We chose these values because they are common K’s that tend to optimize the bias-variance tradeoff. Using leave one out cross validation (LOOCV), we also found the best value of K was 35 neighbors and chose to train the KNN classifier with this value as well. 

**Creating Response Vectors & Design Matrices:**
```{r}
#YTrain is the observed results for admit (0 = not admitted, 1 = admitted)
YTrain <- train$admit
#XTrain is the design matrix after removing admit variable and standardizing 
XTrain <- train[ ,-which(names(train) %in% c("admit"))] %>% 
  scale(center = TRUE, scale = TRUE)

#mean and standard deviation of training set
meanvec <- attr(XTrain,'scaled:center') 
sdvec <- attr(XTrain,'scaled:scale')

#X and Y for testing set
YTest <- test$admit
XTest <- test[ ,-which(names(train) %in% c("admit"))] %>% 
  scale(center = meanvec, scale = sdvec)
```

**Making Predictions on the Test Set with K = 5, 10, 15**
```{r}
#k=5
pred5 = knn(train=XTrain, test=XTest, cl=YTrain, k=5) 
conf.matrix5 <- table(pred=pred5, true=YTest) 
err5 = 1 - sum(diag(conf.matrix5)/sum(conf.matrix5))
#k=10
pred10 = knn(train=XTrain, test=XTest, cl=YTrain, k=10) 
conf.matrix10 <- table(pred=pred10, true=YTest) 
err10 = 1 - sum(diag(conf.matrix10)/sum(conf.matrix10))
#k=20
pred15 = knn(train=XTrain, test=XTest, cl=YTrain, k=15) 
conf.matrix15 <- table(pred=pred15, true=YTest) 
err15 = 1 - sum(diag(conf.matrix15)/sum(conf.matrix15))
```

**Using LOOCV to Find the Best Value of K:**
```{r}
set.seed(150)
#define an empty vector to save validation errors in future
validation.error = NULL
#give possible number of nearest neighbours to be considered 
allK = 1:50
#for each number in allK, use LOOCV to find a validation error 
for (i in allK){
  pred.Yval <- knn.cv(train=XTrain, cl=YTrain, k=i)
  validation.error <- c(validation.error, mean(pred.Yval!=YTrain)) 
}
#best number of neighbors
bestK <- max(allK[validation.error == min(validation.error)]) 
bestK
```

The best value of K to use is 35 because it results in the lowest training error rate.

```{r}
#make predictions on the test set with bestK
pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=bestK) 
#confusion matrix
conf.matrix.knn <- table(pred=pred.YTest, true=YTest) 
conf.matrix.knn
#test error rate
err_bestk = 1 - sum(diag(conf.matrix.knn)/sum(conf.matrix.knn))
err_bestk
```

**Comparing Test Error Rates**
```{r}
knn_results <- matrix(c(err5, err10, err15, err_bestk), ncol=1, byrow=TRUE)
colnames(knn_results) <- c("Test Error Rate")
rownames(knn_results) <- c("K = 5","K = 10","K = 15", "K = 35")
knn_results.table <- as.table(knn_results)
knn_results.table
```

After training all KNN classifiers, we fit the test sets to each model and calculated the test error rates. The model based on 35 nearest neighbors resulted in the lowest misclassification error rate on the test set at 0.1633775. This makes sense since the purpose of LOOCV is to help us find the optimal number of neighbors, and since larger values of K produce smoother decision boundaries with lower variance.

#Logistic Regression

The second basic classifying method we used was logistic regression. Logistic regression is useful when determining variable importance by showing which predictors have the largest impact on our response (admission). 

```{r}
#fit
train.glm <- glm(admit ~ ., data=train, family=binomial) 
summary(train.glm)
```

From the summary output, we see that all four of our predictors are significant to an applicant’s admission based on their low p-values. For a one unit increase in each of these predictor values, the log-odds of getting admitted increases by the specific coefficient amount of that predictor when holding other predictors constant. For example, for a one unit increase in weighted GPA of an applicant, the log-odds of being admitted increases by 3.34.


**Checking How Accurate our Model is with ROC & AUC**

After fitting our logistic regression model on the training set, we constructed a Receiver Operating Characteristic (ROC) curve by plotting the true positive rate (TPR) against the false positive rate (FPR) of our soft classifier and calculated the area under this curve (AUC).

```{r}
train.glm.pred <- predict(train.glm, type="response")
pred = prediction(train.glm.pred, train$admit)
perf = performance(pred, measure="tpr", x.measure="fpr")
#ROC
plot(perf, col=2, lwd=3, main="ROC curve") 
abline(0,1)
#AUC
performance(pred, "auc")@y.values 
```

Since our curve (red line) follows close to the top-left corner of the graph and our AUC, 0.8861, is close to 1, we conclude that logistic regression is a very good classifier for this dataset. If the curve was to closely follow the diagonal (FPR=TPR) and have an area close to 0.5, this would indicate an inaccurate model.

**Using FPR & FNR to Find the Best Threshold Value**

To calculate the test misclassification error rate for this model we need to change the soft classifier (probability values) to a hard classifier (1 or 0) by setting a specific threshold value to assign the labels. We will use our false positive (FPR) and false negative (FNR) results to do this. While still using the training set logistic regression model, we calculated the euclidean distance between each point of (FPR,FNR) and (0, 0).

```{r}
fpr = performance(pred, "fpr")@y.values[[1]] 
threshold = performance(pred, "fpr")@x.values[[1]] 
fnr = performance(pred,"fnr")@y.values[[1]]
#calculating euclidean distance
rate = as.data.frame(cbind(Cutoff=threshold, FPR=fpr, FNR=fnr)) 
rate$distance = sqrt((rate[,2])^2+(rate[,3])^2)
#select threshold with the smallest euclidean distance
index = which.min(rate$distance) 
best = rate$Cutoff[index]
best
```

The best threshold value to use is 0.347139.

```{r}
#plot
matplot(threshold, cbind(fpr,fnr), type="l",lwd=2, xlab="Threshold",ylab="Error Rate") 
legend(0.38, 1, legend=c("False Positive Rate","False Negative Rate"),col=c(1,2), lty=c(1,2))
abline(v=best, col=3, lty=3, lwd=3)
```

```{r}
test.glm <- glm(admit ~ ., data=test, family=binomial)
#soft classifier
test.glm.pred <- predict(test.glm, type="response")
#hard classifier w/ "best" threshold 
test_new = test %>% mutate(admit_pred=as.factor(ifelse(test.glm.pred<=best, 0, 1)))
#confusion matrix
conf.matrix.log <- table(pred=test_new$admit_pred, true=test$admit)
#test error rate
log_error <- 1 - sum(diag(conf.matrix.log)/sum(conf.matrix.log))
log_error
```

With our optimal threshold value, we fit our testing set logistic regression model, constructed a confusion matrix, and found our test misclassification error rate to be 0.1844063. This is slightly greater than our test error rate for K-nearest neighbors.


#Decision Trees
The decision tree algorithm was used because its output is simple to interpret. Each split represents the order in which the tree decides how to further split a set of features to eventually classify a qualitative response. 

To implement the decision tree, we first fit our model with all 4 predictors on the training set. 
```{r}
library(tree)
tree_fit <- tree(admit ~ ., data = train) #fitting classification tree
plot(tree_fit) #plotting tree
text(tree_fit, pretty = 0)
title("Classification Tree Built on Training Set") 
```
However, based on the decision tree’s plot, the data was split using only the gpa.wtd and sati.verb variables. Since decision trees involve looking at each variable’s importance, this indicates that gpa.wtd, followed by sati.verb, were the most significant variables that would help our model gain the most information regarding how to make the best split at each node. 

To calculate the test error rate for our classification tree, we predicted the response values for the test set and then constructed a confusion matrix. 
```{r}
admit.testset <- test$admit
#predict each observation's class using the test set
yhat.testset <- predict(tree_fit, test, type="class") 

#confusion matrix
test_tree_matrix <- table(yhat.testset, admit.testset)
test_tree_matrix
```

Finally, we subtracted the test accuracy rate, which is found through dividing the counter diagonal sum by the total sum, from 1 to get the test error rate.
```{r}
#finding test error rate for this tree
tree_error_rate <- 1-sum(diag(test_tree_matrix))/sum(test_tree_matrix)
tree_error_rate
```
Our decision tree's misclassification error rate is 0.1779. This means that our classification tree was able to correctly predict the admissions decision for 82.21% of the observations in the test set. When compared to KNN, our classification tree algorithm had a slightly higher test error rate, but had a smaller test error rate than logistic regression.

#Random Forest

Another method that was used for our classification was random forest. While containing multiple decision trees makes them harder to interpret, random forests prevent data overfitting and give highly accurate predictions, due to their robustness as an ensemble learning method. 

To implement the classification tree version of a random forest, we fit the model and set the number of parameters as the square root of the total number of predictors. 
```{r}
library(randomForest)
library(tree)
library(gbm)

rf_admits = randomForest(admit ~ ., data=train, ntree=500, importance = TRUE)
plot(rf_admits)
print(rf_admits)
```

Then, we constructed a confusion matrix and calculated the misclassification rate to be 0.1715.
```{r}
yhat.rf = predict(rf_admits, newdata = test)
rf_error = table(pred = yhat.rf, truth = test$admit)
test_rf_error = 1 - sum(diag(rf_error))/sum(rf_error)
test_rf_error
```

Another advantage of using random forests is that they provide information about not only the strength of the relationship between each predictor and response, but also how influential each predictor is on the classifications. 

Each feature's importance is based off of their mean decrease in accuracy and Gini index. To view the degree of influence that each predictor has on the result, we use the importance() function, which displays these 2 factors in decreasing order. 
```{r}
importance(rf_admits)
```
From this output, it tells us that most important variable for getting admitted into the college was weighted GPA, as it had the highest decrease in mean accuracy and Gini index. Verbal SAT score was the 2nd-most important factor that determined the individual's admission decision.

Additionally, we plotted each variable's mean decreases in accuracy and Gini index. 
```{r}
varImpPlot(rf_admits, sort = T, main = "Variable Importance Plot of College Admission Factors")
```
From those graphs, it reemphasizes that the "gpa.wtd" variable had the most influence over the final admissions decision across all trees in the random forest the most, in terms of model accuracy and Gini index.

Overall, random forest had the 2nd-lowest misclassification rate, but did not perform as well as the KNN LOOCV method. 